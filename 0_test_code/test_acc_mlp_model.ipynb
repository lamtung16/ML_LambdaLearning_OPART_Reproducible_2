{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from utility_functions import SquaredHingeLoss\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from utility_functions import SquaredHingeLoss\n",
    "from MLP import MLPModel, mlp_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHs (edit these paths depending on dataset)\n",
    "dataset = 'detailed'\n",
    "\n",
    "# training data\n",
    "fold_path = 'training_data/' + dataset + '/folds.csv'\n",
    "inputs_path = 'training_data/' + dataset + '/inputs.csv'\n",
    "outputs_path = 'training_data/' + dataset + '/outputs.csv'\n",
    "evaluation_path = 'training_data/' + dataset + '/evaluation.csv'\n",
    "\n",
    "# raw dfs\n",
    "fold = 1\n",
    "fold_df = pd.read_csv(fold_path)\n",
    "inputs_df = pd.read_csv(inputs_path)\n",
    "outputs_df = pd.read_csv(outputs_path)\n",
    "evaluation_df = pd.read_csv(evaluation_path)\n",
    "\n",
    "# fold dfs\n",
    "inputs_fold1_df = inputs_df[inputs_df['sequenceID'].isin(fold_df[fold_df['fold'] == 1]['sequenceID'])]\n",
    "outputs_fold1_df = outputs_df[outputs_df['sequenceID'].isin(fold_df[fold_df['fold'] == 1]['sequenceID'])]\n",
    "\n",
    "# feature engineering transformation\n",
    "identity = lambda x: x\n",
    "log      = lambda x: np.log(x)\n",
    "loglog   = lambda x: np.log(np.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_feature = ['length']\n",
    "f_engineering  = [loglog]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_tensor(inputs_df, chosen_feature, f_engineer):\n",
    "    inputs = inputs_df[chosen_feature].to_numpy()\n",
    "    for i in range(len(f_engineer)):\n",
    "        inputs[:, i] = f_engineer[i](inputs[:, i])\n",
    "    inputs = torch.Tensor(inputs)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalzie\n",
    "def normalize_data(tensor):\n",
    "    # Calculate mean and standard deviation along the feature dimension\n",
    "    mean = torch.mean(tensor, dim=0)\n",
    "    std = torch.std(tensor, dim=0)\n",
    "\n",
    "    # Normalize the tensor\n",
    "    normalized_tensor = (tensor - mean) / std\n",
    "\n",
    "    return normalized_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_training(inputs_df, outputs_df, hidden_layers, hidden_size, chosen_feature, f_engineer, normalize, batch_size, margin, n_ites, verbose):\n",
    "    # inputs\n",
    "    inputs = inputs_df[chosen_feature].to_numpy()\n",
    "\n",
    "    # feature engineering\n",
    "    for i in range(len(f_engineer)):\n",
    "        inputs[:, i] = f_engineer[i](inputs[:, i])\n",
    "    inputs = torch.Tensor(inputs)\n",
    "\n",
    "    # normalize input\n",
    "    if normalize == 1:\n",
    "        inputs = normalize_data(inputs)\n",
    "\n",
    "    # outputs\n",
    "    targets_low  = torch.Tensor(outputs_df['min.log.lambda'].to_numpy().reshape(-1,1))\n",
    "    targets_high = torch.Tensor(outputs_df['max.log.lambda'].to_numpy().reshape(-1,1))\n",
    "    outputs = torch.cat((targets_low, targets_high), dim=1)\n",
    "\n",
    "    # prepare training dataset\n",
    "    dataset    = TensorDataset(inputs, outputs)\n",
    "    dataloader = DataLoader(dataset, batch_size, shuffle=False)\n",
    "\n",
    "    # Instantiate model, loss function and optimizer\n",
    "    model = MLPModel(inputs.shape[1], hidden_layers, hidden_size)\n",
    "    criterion = SquaredHingeLoss(margin)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    # Initialize early stopping parameters\n",
    "    best_loss = float('inf')\n",
    "    patience = 5  # Number of epochs to wait before early stopping\n",
    "    num_bad_epochs = 0\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(n_ites):\n",
    "        for features, labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(features), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Calculate validation loss\n",
    "        val_loss = criterion(model(inputs), outputs)\n",
    "        if verbose==1:\n",
    "            print(f\"{epoch}, loss: {val_loss}\")\n",
    "\n",
    "        # Check for early stopping\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            num_bad_epochs = 0\n",
    "        else:\n",
    "            num_bad_epochs += 1\n",
    "            if num_bad_epochs >= patience:\n",
    "                if verbose==1:\n",
    "                    print(f\"Stopping early at epoch {epoch}, loss: {val_loss}\")\n",
    "                break\n",
    "\n",
    "    return model, val_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_criterion(model, inputs_df, outputs_df, chosen_feature, f_engineering):\n",
    "    prediction = model(get_input_tensor(inputs_df, chosen_feature, f_engineering)).detach().numpy().reshape(-1)\n",
    "    min_log = outputs_df['min.log.lambda'].to_numpy()\n",
    "    max_log = outputs_df['max.log.lambda'].to_numpy()\n",
    "    min_margin = prediction - min_log\n",
    "    min_margin[np.isinf(min_margin)] = 0\n",
    "    max_margin = max_log - prediction\n",
    "    max_margin[np.isinf(max_margin)] = 0\n",
    "    criterion = np.sum([min(x, y) for x, y in zip(min_margin, max_margin)])\n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    batch_size  margin  criterion  total_loss\n",
      "3            1     1.5     -8.816    0.805206\n",
      "2            1     1.0     -5.861    0.462787\n",
      "0            1     0.0     -5.278    0.129165\n",
      "1            1     0.5     -4.721    0.253927\n",
      "7          100     1.5     -3.262    0.798821\n",
      "4          100     0.0     -1.732    0.129008\n",
      "6          100     1.0     -1.291    0.459517\n",
      "5          100     0.5     -0.715    0.252645\n",
      "9          600     0.5     -0.144    0.269373\n",
      "8          600     0.0     -0.137    0.142875\n",
      "10         600     1.0     -0.135    0.488784\n",
      "11         600     1.5      0.052    0.829082\n"
     ]
    }
   ],
   "source": [
    "batch_size_candidates = [1, 100, 600]\n",
    "margin_candidates = [0, 0.5, 1, 1.5]\n",
    "\n",
    "def process_params(batch_size, margin):\n",
    "    model, total_loss = mlp_training(inputs_fold1_df, outputs_fold1_df, 0, 0, ['length'], [loglog], 0, batch_size, margin, 500, 0)\n",
    "    criterion = round(get_criterion(model, inputs_fold1_df, outputs_fold1_df, chosen_feature, f_engineering), 3)\n",
    "    return [batch_size, margin, criterion, total_loss]\n",
    "\n",
    "rows = Parallel(n_jobs=-1)(delayed(process_params)(batch_size, margin) \n",
    "                           for batch_size in batch_size_candidates \n",
    "                           for margin in margin_candidates)\n",
    "df = pd.DataFrame(rows, columns=['batch_size', 'margin', 'criterion', 'total_loss'])\n",
    "print(df.sort_values(by='criterion'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size_candidates = [1, 10, 200, 400]\n",
    "# margin_candidates = [0, 1, 2]\n",
    "# n_layer_candidates = [1, 2, 3]\n",
    "# n_neurons_candidates = [4, 8, 16, 32]\n",
    "\n",
    "# def process_params(batch_size, margin, n_layer, n_neurons):\n",
    "#     model, total_loss = mlp_training(inputs_fold1_df, outputs_fold1_df, n_layer, n_neurons, ['length'], [loglog], 1, batch_size, margin, 500, 0)\n",
    "#     criterion = round(get_criterion(model, inputs_fold1_df, outputs_fold1_df, chosen_feature, f_engineering), 3)\n",
    "#     return [batch_size, margin, n_layer, n_neurons, criterion, total_loss]\n",
    "\n",
    "# rows = Parallel(n_jobs=-1)(delayed(process_params)(batch_size, margin, n_layer, n_neurons) \n",
    "#                            for batch_size in batch_size_candidates \n",
    "#                            for margin in margin_candidates \n",
    "#                            for n_layer in n_layer_candidates \n",
    "#                            for n_neurons in n_neurons_candidates)\n",
    "# df = pd.DataFrame(rows, columns=['batch_size', 'margin', 'n_layer', 'n_neurons', 'criterion', 'total_loss'])\n",
    "# print(df.sort_values(by='criterion'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
